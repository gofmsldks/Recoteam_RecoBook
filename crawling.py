# -*- coding: utf-8 -*-
"""crawling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s9EtOoxCjkgb27eA0V41dP5CIhu7KW0e

# yes 24  배스트 셀러 댓글 크롤링
"""

# 구글드라이브 연동
from google.colab import drive
drive.mount('/gdrive', force_remount=True)

# 구글 드라이브 파일 확인
!ls '/gdrive/My Drive/likelion_RecommendationBootChatbot_project2/temp/'

# 반복되는 드라이브 경로 변수화
drive_path = '/gdrive/My Drive/likelion_RecommendationBootChatbot_project2/temp/'

# 설치(20.08 코랩 기준)
!pip install Selenium
!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver

# 한글 폰트 설치
!apt-get install -y fonts-nanum*

import urllib
from urllib.request import urlopen
from urllib.parse import quote_plus
from bs4 import BeautifulSoup as bs
import time
import os
import re
import requests as req
import pandas as pd
import numpy as np

# 셀레니움 설정

from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.alert import Alert
from selenium.webdriver.common.keys import Keys

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless') #내부 창을 띄울 수 없으므로 설정
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
from selenium.webdriver.chrome.options import Options

option = Options()

# 알림창 끄기
option.add_experimental_option("prefs", {
    "profile.default_content_setting_values.notifications": 1
})

# Chrome 드라이버 생성 후 Browser 객체 반환 
wd = webdriver.Chrome('chromedriver', chrome_options=chrome_options)

"""대체로 Selenium에서 a태그를 클릭할 때에는 다음과 같은 방법을 사용합니다.

driver = webdriver.Firefox()

button = driver.find_element_by_css_selector('a')

button.click()



그러나 대략 다음과 같이 링크 대신 자바스크립트를 호출하는 경우에는 click()을 할 수 없는 경우가 있습니다.

<.a href = '#' onclick = 'script'>button</.a>



이런 경우에는 해당 자바스크립트를 직접 실행해야 합니다.

driver.execute_script('script')



경우에 따라서는 엔터를 입력하는 방법이 통할 때도 있습니다.

button.send_keys('\n')

### 광고창 팝업을 닫아야할때 아래 코드 사용
"""

# 광고창 닫기

if len(wd.window_handles) > 1:
    time.sleep(1)
    wd.switch_to_window(wd.window_handles[1])
    wd.close()

    wd.switch_to_window(wd.window_handles[0])

"""### 셀레니움으로 다운로드 받은 다음에 이름 바꾸는 법"""

for downloadlink, uniqueName in my_list_of_links_and_names:
    driver = webdriver.Chrome();
    driver.get(downloadurl)
    time.sleep(5) # give it time to download (not sure if this is necessary)
    # the file is now downloaded
    os.rename('국내도서종합_YES베스트.xls', '국내도서종합_YES베스트1.xls') # the name is now changed

headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
        'referer':'https://nid.naver.com/login/sso/finalize.nhn?url=https%3A%2F%2Fwww.naver.com&sid=KBwWbyEJdJL521l3&svctype=1'}

# 전체 랭크 카운팅
all_rank = 1
# 셀레니움 사용 
frank = 1

id = []
name = []
writer = []
launch_date = []
category = []
rank = []
score = []
comment = []


# 전체 페이지
for i in range(2):
    url = f'http://www.yes24.com/24/category/bestseller?CategoryNumber=001&sumgb=06&PageNumber={i+1}&FetchSize=80'
    wd.get(url)
    print('*********현재페이지:', url, '**********')
    # 도서정보 xls 파일 가지고 오는 부분
    lrank = (i+1)*80 # 마지막 링킹

    # 팝업창 닫기
    if len(wd.window_handles) > 1:
        time.sleep(1)
        wd.switch_to_window(wd.window_handles[1])
        wd.close()
        wd.switch_to_window(wd.window_handles[0])


    #download_exel = wd.find_element_by_css_selector('#bestList > div:nth-child(6) > div.boxSortBot > div.sortRgt > a:nth-child(4)')
    #download_exel.click()
    #time.sleep(3)
    #os.rename('국내도서종합_YES베스트.xls', f'국내도서종합_YES베스트_{frank}_{lrank}.xls')
    frank = lrank+1 # (마지막 랭킹 + 1) 을 첫번째 링킹으로 대체

    print(f'*********{i+1}번째 페이지 크롤링 시작**********')

    # print('*********랭킹에 속해있는 도서의 xls 파일을 저장합니다.**********')

    # 도서 80권수 만큼 반복(한 페이지당)
    for rank_index in range(5):
        
        # 팝업창 닫기
        if len(wd.window_handles) > 1:
            time.sleep(1)
            wd.switch_to_window(wd.window_handles[1])
            wd.close()
            wd.switch_to_window(wd.window_handles[0])

        # 제목 또는 댓글 보기 클릭(제목 링크 클릭하는 것으로 일단 구현)
        # view_comment = wd.find_element_by_css_selector(f'#category_layout > tbody > tr:nth-child({rank_index*2+1}) > td.goodsTxtInfo > p:nth-child(1) > a:nth-child(1)')
        # view_comment = wd.find_element_by_xpath(f'//*[@id="category_layout"]/tbody/tr[{i*2+1}]/td[3]/p[1]/a[1]')

        view = wd.find_element_by_xpath(f'//*[@id="category_layout"]/tbody/tr[{rank_index*2+1}]/td[2]/div/a')

        # view = wd.find_element_by_xpath(f'//*[@id="category_layout"]/tbody/tr[{rank_index*2+1}]/td[3]/p[1]/a[1]')

        if i == 1:
            time.sleep(3)
            wd.save_screenshot(f'왜 안돼.png')

        view.click()

        time.sleep(3)
        wd.save_screenshot(f'view_detail{rank_index}.png')

        view_url = wd.current_url
        print(view_url)

        # book id만 꺼내서 한줄평 링크(url_review_base) 맨 끝에다 붙이기
        book_id = view_url.split('/')[5]
        url_review_base = f'http://www.yes24.com/Product/communityModules/AwordReviewList/{book_id}'


        # 각 도서의 한줄평 댓글 부분(페이지 수 만큼 for문으로 돌리기)
        for page_index in range(1):
            # url_review_base에 페이지 넘버 붙이기
            # 한줄평 댓글 내용 크롤링
            mention_comment_url = url_review_base + f'?PageNumber={page_index+1}'
            print(mention_comment_url)

            # 도서 한줄평 페이지 불러오기
            res = req.get(mention_comment_url).text
            soup = bs(res, 'html.parser')

            # 도서 view 페이지 불러오기
            res2 =  req.get(view_url).text
            soup2 = bs(res2, 'html.parser')

            names = soup2.find("h2","gd_name").string
            categories = soup2.select('#infoset_goodsCate > div.infoSetCont_wrap > dl a')
            writers = soup2.select("#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_auth > a")
            launch_dates = soup2.select("#infoset_specific > div.infoSetCont_wrap > div > table > tbody > tr td")
            comments = soup.findAll("span","txt")
            scores = soup.findAll("span", class_ = "rating")
            
            print(names)
            print(writers[0].text)
            print(categories)
            print(launch_dates[0].text)
            print(comments)
            print(scores)

            temp_category = []
            for c in range(len(categories)):
                temp_category.append(categories[c].text)

            for k in range(len(comments)):
                id.append(book_id)
                name.append(names)
                writer.append(writers[0].text)
                launch_date.append(launch_dates[0].text)
                rank.append(all_rank)
                score.append(scores[k].text)
                comment.append(comments[k].text)
                category.append(temp_category)

        all_rank += 1


        # 뒤로 돌아가기
        wd.back()
        time.sleep(3)
        wd.save_screenshot(f'view_back{rank_index}.png')


comment_df = pd.DataFrame({'id': id, 'name' : name, '출판일': launch_date, '분류': category, '작가': writer, '순위': rank, '평점': score, '한줄평': comment})
print(comment_df)
comment_df.to_csv(f"1_{(i+1)*80}_순위별한줄평.csv")

comment_url = wd.current_url
        print(comment_url)
        res = req.get(comment_url).text
        soup = bs(res, 'html.parser')

url = 'http://www.yes24.com/24/category/bestseller?CategoryNumber=001&sumgb=06&PageNumber=1&FetchSize=80'
    wd.get(url)
    view = wd.find_element_by_xpath('//*[@id="category_layout"]/tbody/tr[9]/td[3]/p[1]/a[1]')
    view.click()
    time.sleep(3)

    wd.save_screenshot(f'클릭테스트{rank_index}.png')

for page_index in range(1):
            # url_review_base에 페이지 넘버 붙이기
            # 한줄평 댓글 내용 크롤링
            mention_comment_url = url_review_base + f'?PageNumber={page_index+1}'
            print(mention_comment_url)

            res = req.get(mention_comment_url).text
            soup = bs(res, 'html.parser')

            names = soup.find("title")
            categories = soup.find()
            writers = soup.findAll("span","gd_auth")
            launch_dates = soup.findAll("span","gd_date")
            comments = soup.findAll("span","txt")
            scores = soup.findAll("span", class_ = "rating")
            
            print(names)
            print(writers)
            print(categories)
            print(launch_dates)
            print(comments)
            print(scores)

            for k in range(len(comments)):
                id.append(book_id)
                name.append(names)
                writer.append(writers)
                category.append(categories)
                launch_date.append(launch_dates)
                rank.append(all_rank)
                score.append(scores[k].text)
                comment.append(comments[k].text)