# -*- coding: utf-8 -*-
"""ë„¤ì´ë²„ì‡¼í•‘_ì˜í™”ë¦¬ë·°í•™ìŠµëª¨ë¸_(ë„ì„œë°ì´í„°ë„í•™ìŠµëª¨ë¸ë§Œë“¤ì˜ˆì •).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17UeaQKAYz2yoqz-CWOJWkHmeqlCbQD0D
"""

# êµ¬ê¸€ë“œë¼ì´ë¸Œ ì—°ë™
from google.colab import drive
drive.mount('/gdrive', force_remount=True)

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ í™•ì¸
!ls '/gdrive/MyDrive/likelion_Recoteam_RecoBook_project2/temp/'

# ë°˜ë³µë˜ëŠ” ë“œë¼ì´ë¸Œ ê²½ë¡œ ë³€ìˆ˜í™”
drive_path = '/gdrive/MyDrive/likelion_Recoteam_RecoBook_project2/temp/'

import os
os.chdir(drive_path)
print(os.getcwd())

!pip install nltk
import nltk
nltk.download('treebank')

!pip install konlpy

"""1. ì»¨í…ì¸  ê¸°ë°˜ ì¶”ì²œ


* ì»¨í…ì¸ ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œ(ê°€ì¥ ê°„ë‹¨)
* ex) ì¸í’‹: ì‚¬ìš©ìê°€ ê·€ë©¸ì˜ ì¹¼ë‚ , ë§Œí™”1, ë§Œí™”2ë¥¼ ì…ë ¥
* ë¨¼ì € ì…ë ¥í•œ ì±… ì´ë¦„ì„ ë°”íƒ•ìœ¼ë¡œ ë„ì„œ ë°ì´í„° ì…‹ì—ì„œ ì¥ë¥´, ê°ì •ì ìˆ˜, í‰ì ì„ ì°¾ì•„ë‚´ê³ 
* ë¹„ìŠ·í•œ ìš”ì†Œë“¤ì„ ì—°ê´€ë„ ë¶„ì„ì„ í†µí•´ ë½‘ì•„ë‚´ê³  ì—¬ê¸°ì„œ ìœ ì‚¬í•œ ì¥ë¥´, ê°ì •, í‰ì ì„ ê°€ì§„ ì±…ì„ ë½‘ì•„ë‚´ê¸°


---------------


2. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê°ì • ìš”ì†Œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œ(ë ì§€ ì•ˆë ì§€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤)

* í•œì¤„í‰ì„ ê°ì •ë¶„ì„í•˜ê³  ê°ì •ë¶„ì„í•œê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ êµ°ì§‘í™”(ê¸ì • êµ°ì§‘, ë¶€ì • êµ°ì§‘...etc)
* ì¸í’‹: ì‚¬ìš©ìê°€ 'ê°ë™: 40, ì¬ë¯¸: 60'ë¥¼ ì›í•œë‹¤ê³  ì…ë ¥  
* 'ê°ë™ êµ°ì§‘'ì—ì„œ í‰ì  ë†’ì€ ì±… 4ê¶Œ ì¶”ì¶œ, 'ì¬ë¯¸ êµ°ì§‘'ì—ì„œ í‰ì  ë†’ì€ ì±… 6ê¶Œ ì¶”ì¶œ.  
* ì•„ì›ƒí’‹: ì¶”ì¶œí•œ ì±…ë“¤ ëŒ“ê¸€ ê°ì • ë¶„ì„í•œê±° í¼ì„¼íŠ¸ ë¹„ì¤‘ íŒŒì´ì°¨íŠ¸ë¡œ í‘œí˜„ í•˜ê³  ì¼ì¹˜í•˜ëŠ” ì±…ë“¤ ë³´ì—¬ì£¼ê¸°


---------------


3. ì•„ì´í…œ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§(ìœ ì € ê°œì¸ì— íŠ¹í™”ëœ ì¶”ì²œë°©ì‹)

* ì…ë ¥: ì‚¬ì „ì— ì‚¬ìš©ìê°€ ì´ì „ì— ì½ì—ˆë˜ êµ­ë‚´ ë² ìŠ¤íŠ¸ ì…€ëŸ¬ì˜ ì´ë¦„ì„ ëª‡ê°€ì§€ ì…ë ¥í•˜ê³  ê¸ì • ë¶€ì • ë“±ë“±ì˜ ê°ì • í¼ì„¼íŠ¸ë¥¼ ì…ë ¥í•˜ê²Œ í•¨.  
* ê¸°ì¡´ í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë§Œë“¦
*  í–‰: userID , ì—´: ì±… ì¢…ë¥˜ (í–‰ì—´ ë°”ë€Œì–´ë„ ë¬´ê´€) ë‚´ìš©ì€ ê°ì • ë¶„ì„í•œ ì ìˆ˜ë¡œ  
* ë°ì´í„° í”„ë ˆì„ì— ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì •ë³´ë“¤ ì¶”ê°€ 
*  ì•„ì›ƒí’‹:  surpriseë¥¼ ì‚¬ìš©í•´ì„œ êµ¬í•œ ê°’
* ë” ì‹¬í™”í•˜ë ¤ë©´ í–‰ë ¬ ë¶„í•´ê¸°ë°˜ ì ì¬ìš”ì¸ í˜‘ì—… í•„í„°ë§


---------------


* í˜‘ì—… í•„í„°ë§ (Collaborative Filtering, CF)ì´ë€ ì—¬ëŸ¬ ì‚¬ìš©ìë“¤ë¡œë¶€í„° ì–»ì€ ê¸°í˜¸ ì •ë³´ì— ë”°ë¼ ë‹¤ë¥¸ ì‚¬ìš©ìë“¤ì˜ ê´€ì‹¬ì‚¬ë¥¼ ì˜ˆì¸¡í•˜ê²Œ í•´ì£¼ëŠ” ë°©ë²• ì´ë¼ê³  ì •ì˜ë©ë‹ˆë‹¤.
 
* í˜‘ì—… í•„í„°ë§ì—ì„œ ì¤‘ìš”í•œ ê²ƒì€ "ì—¬ëŸ¬ ì‚¬ìš©ìë“¤ë¡œë¶€í„° ì–»ì€ ì •ë³´"ì…ë‹ˆë‹¤. 
 
* í˜‘ì—… í•„í„°ë§ì—ëŠ” í¬ê²Œ [ì‚¬ìš©ì ê¸°ë°˜ ì¶”ì²œ, ì•„ì´í…œ ê¸°ë°˜ ì¶”ì²œ] ë‘ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.
 
* 1) ì‚¬ìš©ì ê¸°ë°˜ ì¶”ì²œ (User-based Recommendation)
ë¹„ìŠ·í•œ ì„±í–¥ì„ ì§€ë‹Œ ì‚¬ìš©ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•´ì„œ ì¶”ì²œí•´ì£¼ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
 
* Aë¼ëŠ” ì‚¬ëŒì´ [í–„ë²„ê±°, ê°ìíŠ€ê¹€, ì½œë¼]ë¥¼ êµ¬ë§¤í•˜ê³ ,
* Bë¼ëŠ” ì‚¬ëŒì€ [í–„ë²„ê±°, ì½œë¼]ë¥¼ êµ¬ë§¤í•˜ë ¤ í•œë‹¤ê³  ê°€ì •í•´ë³´ê² ìŠµë‹ˆë‹¤.
 
* ì´ ë‘˜ì˜ êµ¬ë§¤ëª©ë¡ì„ ë³´ë©´ ì´ ë‘˜ì€ ìœ ì‚¬í•˜ë‹¤ê³  ì¸ì‹ë˜ì–´ Bì—ê²Œ ê°ìíŠ€ê¹€ì„ ì¶”ì²œí•´ì¤ë‹ˆë‹¤.
 
* 2) ì•„ì´í…œ ê¸°ë°˜ ì¶”ì²œ (Item-based Recommendation)
* ì´ì „ì— êµ¬ë§¤í–ˆë˜ ì•„ì´í…œì„ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ ìƒí’ˆì„ ì¶”ì²œí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
 
* ì˜ˆë¥¼ ë“¤ì–´ ê¸°ì¡´ì— [í–„ë²„ê±°, ê°ìíŠ€ê¹€]ì´ í•¨ê»˜ êµ¬ë§¤ë˜ëŠ” ë¹ˆë„ê°€ ë§ë‹¤ê³  ë¶„ì„ë˜ë©´ ì´ ë‘˜ì€ ìœ ì‚¬ë„ê°€ ë†’ë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤.
* ë”°ë¼ì„œ Cë¼ëŠ” ì‚¬ëŒì´ í–„ë²„ê±°ë¥¼ êµ¬ë§¤í•˜ë ¤ê³  í•˜ë©´ ìœ ì‚¬í•œ ì•„ì´í…œìœ¼ë¡œ íŒë‹¨ë˜ëŠ” ê°ìíŠ€ê¹€ì„ ì¶”ì²œí•´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.

# ì»¨í…ì¸  ê¸°ë°˜ ì¶”ì²œ

## 1. ëŒ“ê¸€ ê°ì •ë¶„ì„

ë°©ì•ˆ1.
* ëŒ“ê¸€ì˜ ê°ì •ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë¯¸ë¦¬ í•™ìŠµ ì‹œí‚¬ ë°ì´í„°ê°€ í•„ìš”.
* 1. ë„¤ì´ë²„ ê¸°ì‚¬ ëŒ“ê¸€, ë‰´ìŠ¤ ëŒ“ê¸€ ì‚¬ìš©í•´ì„œ ëª¨ë¸1ì„ í•™ìŠµ
* 2. í•™ìŠµëœ ëª¨ë¸1ì„ ì‚¬ìš©í•´ì„œ ë„ì„œ ëŒ“ê¸€ ì •ë³´ ë¼ë²¨ë§
* 3. ë¼ë²¨ë§ëœ ë„ì„œ ëŒ“ê¸€ ì •ë³´ë¥¼ train, testë¡œ ë‚˜ëˆ„ì–´ì„œ ë‹¤ì‹œ í•œë²ˆ training ì‹œí‚¤ê¸°


ë°©ì•ˆ2.
* 1. ëŒ“ê¸€ ë°ì´í„°ì™€ í‰ì  ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í† í¬ë‚˜ì´ì§• ë° k-means êµ°ì§‘í™”
* 2. êµ°ì§‘í™”ë¥¼ í†µí•´ ë¼ë²¨ë§ ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ

# ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê¸ì • ë¶€ì • ëª¨ë¸
"""

# Commented out IPython magic to ensure Python compatibility.
# ë°©ì•ˆ1

import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
import re
import urllib.request
from konlpy.tag import Okt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt", filename="movie_ratings_train.txt")
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt", filename="movie_ratings_test.txt")

train_data = pd.read_table('movie_ratings_train.txt')
test_data = pd.read_table('movie_ratings_test.txt')

train_data.info()

print('í›ˆë ¨ìš© ë¦¬ë·° ê°œìˆ˜ :',len(train_data)) # í›ˆë ¨ìš© ë¦¬ë·° ê°œìˆ˜ ì¶œë ¥
print(train_data[:5]) # train ìƒìœ„ 5ê°œ ì¶œë ¥

print('---------------------------------------------------------------------------------------------------\n')

print('í…ŒìŠ¤íŠ¸ìš© ë¦¬ë·° ê°œìˆ˜ :',len(test_data)) # í…ŒìŠ¤íŠ¸ìš© ë¦¬ë·° ê°œìˆ˜ ì¶œë ¥
print(test_data[:5]) # test ìƒìœ„ 5ê°œ ì¶œë ¥


print('---------------------------------------------------------------------------------------------------\n')

train_data.drop_duplicates(subset=['document'], inplace=True) # document ì—´ì—ì„œ ì¤‘ë³µì¸ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì¤‘ë³µ ì œê±°
print('ì´ ìƒ˜í”Œì˜ ìˆ˜ :',len(train_data))
train_data['label'].value_counts().plot(kind = 'bar')
print(train_data.groupby('label').size().reset_index(name = 'count'))

train_data = train_data.dropna(how = 'any') # Null ê°’ì´ ì¡´ì¬í•˜ëŠ” í–‰ ì œê±°
print('null ì¡´ì¬?:', train_data.isnull().values.any()) # Null ê°’ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸

print('---------------------------------------------------------------------------------------------------\n')

print('///í•œê¸€ê³¼ ê³µë°± ì™¸ì˜ ê²ƒ ëª¨ë‘ ì œê±°///')
train_data['document'] = train_data['document'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")
# í•œê¸€ê³¼ ê³µë°±ì„ ì œì™¸í•˜ê³  ëª¨ë‘ ì œê±°
print(train_data[:5])

train_data['document'] = train_data['document'].str.replace('^ +', "") # white space ë°ì´í„°ë¥¼ empty valueë¡œ ë³€ê²½
train_data['document'].replace('', np.nan, inplace=True)
print(train_data.isnull().sum())

train_data = train_data.dropna(how = 'any') # nan ì œê±°
print(len(train_data))

test_data.drop_duplicates(subset = ['document'], inplace=True) # document ì—´ì—ì„œ ì¤‘ë³µì¸ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì¤‘ë³µ ì œê±°
test_data['document'] = test_data['document'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","") # ì •ê·œ í‘œí˜„ì‹ ìˆ˜í–‰
test_data['document'] = test_data['document'].str.replace('^ +', "") # ê³µë°±ì€ empty ê°’ìœ¼ë¡œ ë³€ê²½
test_data['document'].replace('', np.nan, inplace=True) # ê³µë°±ì€ Null ê°’ìœ¼ë¡œ ë³€ê²½
test_data = test_data.dropna(how='any') # Null ê°’ ì œê±°
print('ì „ì²˜ë¦¬ í›„ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œì˜ ê°œìˆ˜ :',len(test_data))

okt = Okt()
stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']

X_train = []
for sentence in train_data['document']:
    temp_X = okt.morphs(sentence, stem=True) # í† í°í™”
    temp_X = [word for word in temp_X if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°
    X_train.append(temp_X)

print(X_train[:3])


X_test = []
for sentence in test_data['document']:
    temp_X = okt.morphs(sentence, stem=True) # í† í°í™”
    temp_X = [word for word in temp_X if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°
    X_test.append(temp_X)

import pickle
with open('movie_X_train.txt', 'wb') as f:
    pickle.dump(X_train, f)
    
with open('movie_X_test.txt', 'wb') as f:
    pickle.dump(X_test, f)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

print(tokenizer.word_index)

with open('movie_X_train.txt', 'rb') as f:
    movie_X_train = pickle.load(f) # ë‹¨ í•œì¤„ì”© ì½ì–´ì˜´
movie_X_train

threshold = 3
total_cnt = len(tokenizer.word_index) # ë‹¨ì–´ì˜ ìˆ˜
rare_cnt = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸
total_freq = 0 # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©
rare_freq = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©

# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :',total_cnt)
print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))
print("ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:", (rare_cnt / total_cnt)*100)
print("ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:", (rare_freq / total_freq)*100)



# ì „ì²´ ë‹¨ì–´ ê°œìˆ˜ ì¤‘ ë¹ˆë„ìˆ˜ 2ì´í•˜ì¸ ë‹¨ì–´ëŠ” ì œê±°.
# 0ë²ˆ íŒ¨ë”© í† í°ì„ ê³ ë ¤í•˜ì—¬ + 1
vocab_size = total_cnt - rare_cnt + 1
print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° :',vocab_size)

tokenizer = Tokenizer(vocab_size) 
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

y_train = np.array(train_data['label'])
y_test = np.array(test_data['label'])

drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]


# ë¹ˆ ìƒ˜í”Œë“¤ì„ ì œê±°
X_train = np.delete(X_train, drop_train, axis=0)
y_train = np.delete(y_train, drop_train, axis=0)
print(len(X_train))
print(len(y_train))

print('ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ :',max(len(l) for l in X_train))
print('ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ :',sum(map(len, X_train))/len(X_train))
plt.hist([len(s) for s in X_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

def below_threshold_len(max_len, nested_list):
  cnt = 0
  for s in nested_list:
    if(len(s) <= max_len):
        cnt = cnt + 1
  print('ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: %s'%(max_len, (cnt / len(nested_list))*100))

max_len = 30
below_threshold_len(max_len, X_train)

X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)

"""## LSTM ëª¨ë¸ ì ìš©"""

from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

model = Sequential()
model.add(Embedding(vocab_size, 100))
model.add(LSTM(128))
model.add(Dense(1, activation='sigmoid'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)

loaded_model = load_model('naver_movie_best_model.h5')
print("\n í…ŒìŠ¤íŠ¸ ì •í™•ë„: %.4f" % (loaded_model.evaluate(X_test, y_test)[1]))

"""## í•™ìŠµëœ ëª¨ë¸ ì ìš©"""

def sentiment_predict(new_sentence):
  new_sentence = okt.morphs(new_sentence, stem=True) # í† í°í™”
  new_sentence = [word for word in new_sentence if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°
  encoded = tokenizer.texts_to_sequences([new_sentence]) # ì •ìˆ˜ ì¸ì½”ë”©
  pad_new = pad_sequences(encoded, maxlen = max_len) # íŒ¨ë”©
  score = float(loaded_model.predict(pad_new)) # ì˜ˆì¸¡
  if(score > 0.5):
    print("{:.2f}% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n".format(score * 100))
  else:
    print("{:.2f}% í™•ë¥ ë¡œ ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n".format((1 - score) * 100))

# í…ŒìŠ¤íŠ¸
sentiment_predict('ì´ ì±… ì•„ì£¼ ì¬ë°Œì–´')
sentiment_predict('ì™€ ê°œì©ë‹¤ ì •ë§ ì„¸ê³„ê´€ ìµœê°•ìë“¤ì˜ ì˜í™”ë‹¤')
sentiment_predict('ê°ë… ë­í•˜ëŠ” ë†ˆì´ëƒ?')
sentiment_predict('ì´ë”´ê²Œ ì˜í™”ëƒ ã…‰ã…‰')
sentiment_predict('ì´ ì˜í™” í•µë…¸ì¼ ã… ã… ')

# ë„ì„œ ë¦¬ë·° ë¶ˆëŸ¬ì˜¤ê¸°

"""## ì˜í™” ë¦¬ë·° í† í¬ë‚˜ì´ì§•, ëª¨ë¸ ë§Œë“ ê²ƒì„ ë°”ë¡œ ì ìš©í•´ì£¼ê²Œ í†µí•©
* ìœ„ì˜ í•™ìŠµ ê³¼ì •ì„ ê±°ì¹˜ê³  ì´ ë¸”ë¡ì„ ì‹¤í–‰í•˜ë©´ ë¨.
* ì¶”í›„ì— ë„ì„œ ë¦¬ë·°ë°ì´í„°ë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ë¡œ ë§Œë“¦
* ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆê²Œ êµ¬ì„±
* ì´ ëª¨ë“ˆì„ í™œìš©í•´ì„œ ê° ë„ì„œ ëŒ“ê¸€ì„ ê°ì„±ë¶„ì„í•˜ê³  ê°ì„± ë¶„ì„ í•œ ê²°ê³¼ë¥¼ ì €ì¥
* ì´ ê²°ê³¼ë¥¼ í† ëŒ€ë¡œ ì‚¬ìš©ìê°€ í•´ë‹¹ ë„ì„œë¥¼ ì…ë ¥í–ˆì„ ê²½ìš° í•´ë‹¹ ë„ì„œì˜ ê¸ë¶€ì • í¼ì„¼íŠ¸ë¥¼ ë³´ì—¬ì¤Œ
"""

# Commented out IPython magic to ensure Python compatibility.
import os
os.chdir(drive_path)
print(os.getcwd())

from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
import re
import urllib.request
from konlpy.tag import Okt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
# í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©

# ì „ì²˜ë¦¬ë§Œ í•˜ê¸°(ì „ì²´ ë°ì´í„° ëŒ€ìƒ)
train_data = pd.read_table('movie_ratings_train.txt')
test_data = pd.read_table('movie_ratings_test.txt')
train_data.drop_duplicates(subset=['document'], inplace=True) # document ì—´ì—ì„œ ì¤‘ë³µì¸ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì¤‘ë³µ ì œê±°
print('ì´ ìƒ˜í”Œì˜ ìˆ˜ :',len(train_data))


print(train_data.groupby('label').size().reset_index(name = 'count'))
train_data = train_data.dropna(how = 'any') # Null ê°’ì´ ì¡´ì¬í•˜ëŠ” í–‰ ì œê±°
print('null ì¡´ì¬?:', train_data.isnull().values.any()) # Null ê°’ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸

print('---------------------------------------------------------------------------------------------------\n')

print('///í•œê¸€ê³¼ ê³µë°± ì™¸ì˜ ê²ƒ ëª¨ë‘ ì œê±°///')
train_data['document'] = train_data['document'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")
# í•œê¸€ê³¼ ê³µë°±ì„ ì œì™¸í•˜ê³  ëª¨ë‘ ì œê±°
print(train_data[:5])
train_data['document'] = train_data['document'].str.replace('^ +', "") # white space ë°ì´í„°ë¥¼ empty valueë¡œ ë³€ê²½
train_data['document'].replace('', np.nan, inplace=True)
print(train_data.isnull().sum())

train_data = train_data.dropna(how = 'any') # nan ì œê±°
print(len(train_data))
test_data.drop_duplicates(subset = ['document'], inplace=True) # document ì—´ì—ì„œ ì¤‘ë³µì¸ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì¤‘ë³µ ì œê±°
test_data['document'] = test_data['document'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","") # ì •ê·œ í‘œí˜„ì‹ ìˆ˜í–‰
test_data['document'] = test_data['document'].str.replace('^ +', "") # ê³µë°±ì€ empty ê°’ìœ¼ë¡œ ë³€ê²½
test_data['document'].replace('', np.nan, inplace=True) # ê³µë°±ì€ Null ê°’ìœ¼ë¡œ ë³€ê²½
test_data = test_data.dropna(how='any') # Null ê°’ ì œê±°
print('ì „ì²˜ë¦¬ í›„ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œì˜ ê°œìˆ˜ :',len(test_data))

okt = Okt()
stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']

# ì „ì²˜ë¦¬ í•˜ê³  í† í¬ë‚˜ì´ì§• í•œ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°(ì „ì²˜ë¦¬ + í† í¬ë‚˜ì´ì§•ì€ ì‹œê°„ì´ ë§ì´ ê±¸ë ¤ ì‘ì—… ìˆ˜í–‰í•œ íŒŒì¼ì„ pickleì— ì €ì¥í•œ í›„ ë¶ˆëŸ¬ì˜´)

with open('movie_X_train.txt', 'rb') as f:
    movie_X_train = pickle.load(f) # ë‹¨ í•œì¤„ì”© ì½ì–´ì˜´


with open('movie_X_test.txt', 'rb') as f:
    movie_X_test = pickle.load(f) # ë‹¨ í•œì¤„ì”© ì½ì–´ì˜´

tokenizer = Tokenizer()
tokenizer.fit_on_texts(movie_X_train)

print(tokenizer.word_index)

threshold = 3
total_cnt = len(tokenizer.word_index) # ë‹¨ì–´ì˜ ìˆ˜
rare_cnt = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸
total_freq = 0 # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©
rare_freq = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©

# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :',total_cnt)
print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))
print("ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:", (rare_cnt / total_cnt)*100)
print("ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:", (rare_freq / total_freq)*100)


# ì „ì²´ ë‹¨ì–´ ê°œìˆ˜ ì¤‘ ë¹ˆë„ìˆ˜ 2ì´í•˜ì¸ ë‹¨ì–´ëŠ” ì œê±°.
# 0ë²ˆ íŒ¨ë”© í† í°ì„ ê³ ë ¤í•˜ì—¬ + 1
vocab_size = total_cnt - rare_cnt + 1
print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° :',vocab_size)



tokenizer = Tokenizer(vocab_size) 
tokenizer.fit_on_texts(movie_X_train)
X_train = tokenizer.texts_to_sequences(movie_X_train)
X_test = tokenizer.texts_to_sequences(movie_X_test)

y_train = np.array(train_data['label'])
y_test = np.array(test_data['label'])


drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]


# ë¹ˆ ìƒ˜í”Œë“¤ì„ ì œê±°
X_train = np.delete(X_train, drop_train, axis=0)
y_train = np.delete(y_train, drop_train, axis=0)
print(len(X_train))
print(len(y_train))


def below_threshold_len(max_len, nested_list):
  cnt = 0
  for s in nested_list:
    if(len(s) <= max_len):
        cnt = cnt + 1
  print('ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: %s'%(max_len, (cnt / len(nested_list))*100))


max_len = 30
below_threshold_len(max_len, X_train)

X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)

loaded_model = load_model('naver_movie_best_model.h5')


def sentiment_predict(new_sentence):
  new_sentence = okt.morphs(new_sentence, stem=True) # í† í°í™”
  new_sentence = [word for word in new_sentence if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°
  encoded = tokenizer.texts_to_sequences([new_sentence]) # ì •ìˆ˜ ì¸ì½”ë”©
  pad_new = pad_sequences(encoded, maxlen = max_len) # íŒ¨ë”©
  score = float(loaded_model.predict(pad_new)) # ì˜ˆì¸¡
  
  if(score > 0.5):
    print("{:.2f}% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.".format(score * 100))
  else:
    print("{:.2f}% í™•ë¥ ë¡œ ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤.".format((1 - score) * 100))

  return (score, 1-score)



# ë„ì„œ ë¦¬ë·° ë¶ˆëŸ¬ì˜¤ê¸°

# í…ŒìŠ¤íŠ¸
sentiment_predict('ì´ ì±… ì•„ì£¼ ì¬ë°Œì–´')
sentiment_predict('ì™€ ê°œì©ë‹¤ ì •ë§ ì„¸ê³„ê´€ ìµœê°•ìë“¤ì˜ ì±…ì´ë‹¤')
sentiment_predict('ì‘ê°€ ë­í•˜ëŠ” ë†ˆì´ëƒ?')
sentiment_predict('ì´ë”´ê²Œ ì±…ì´ëƒ ã…‰ã…‰')
sentiment_predict('ì´ ì±… í•µë…¸ì¼ ã… ã… ')

"""# ë„¤ì´ë²„ ì‡¼í•‘ ë¦¬ë·° ê°ì„± ë¶„ì„"""

# Commented out IPython magic to ensure Python compatibility.
# Colabì— Mecab ì„¤ì¹˜
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
# %cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from collections import Counter
from konlpy.tag import Mecab
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

urllib.request.urlretrieve("https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt", filename="shopping_ratings_total.txt")

total_data = pd.read_table('shopping_ratings_total.txt', names=['ratings', 'reviews'])
print('ì „ì²´ ë¦¬ë·° ê°œìˆ˜ :',len(total_data)) # ì „ì²´ ë¦¬ë·° ê°œìˆ˜ ì¶œë ¥

total_data['label'] = np.select([total_data.ratings > 3], [1], default=0)
total_data[:5]

train_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)
print('í›ˆë ¨ìš© ë¦¬ë·°ì˜ ê°œìˆ˜ :', len(train_data))
print('í…ŒìŠ¤íŠ¸ìš© ë¦¬ë·°ì˜ ê°œìˆ˜ :', len(test_data))

# í•œê¸€ê³¼ ê³µë°±ì„ ì œì™¸í•˜ê³  ëª¨ë‘ ì œê±°
train_data['reviews'] = train_data['reviews'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")
train_data['reviews'].replace('', np.nan, inplace=True)
print(train_data.isnull().sum())

test_data.drop_duplicates(subset = ['reviews'], inplace=True) # ì¤‘ë³µ ì œê±°
test_data['reviews'] = test_data['reviews'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","") # ì •ê·œ í‘œí˜„ì‹ ìˆ˜í–‰
test_data['reviews'].replace('', np.nan, inplace=True) # ê³µë°±ì€ Null ê°’ìœ¼ë¡œ ë³€ê²½
test_data = test_data.dropna(how='any') # Null ê°’ ì œê±°
print('ì „ì²˜ë¦¬ í›„ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œì˜ ê°œìˆ˜ :',len(test_data))

mecab = Mecab()
stopwords = ['ë„', 'ëŠ”', 'ë‹¤', 'ì˜', 'ê°€', 'ì´', 'ì€', 'í•œ', 'ì—', 'í•˜', 'ê³ ', 'ì„', 'ë¥¼', 'ì¸', 'ë“¯', 'ê³¼', 'ì™€', 'ë„¤', 'ë“¤', 'ë“¯', 'ì§€', 'ì„', 'ê²Œ']

train_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)
train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])
test_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)
test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])

negative_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)
positive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)

negative_word_count = Counter(negative_words)
print(negative_word_count.most_common(20))

positive_word_count = Counter(positive_words)
print(positive_word_count.most_common(20))

fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
text_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))
ax1.hist(text_len, color='red')
ax1.set_title('Positive Reviews')
ax1.set_xlabel('length of samples')
ax1.set_ylabel('number of samples')
print('ê¸ì • ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ :', np.mean(text_len))

text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))
ax2.hist(text_len, color='blue')
ax2.set_title('Negative Reviews')
fig.suptitle('Words in texts')
ax2.set_xlabel('length of samples')
ax2.set_ylabel('number of samples')
print('ë¶€ì • ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ :', np.mean(text_len))
plt.show()

X_train = train_data['tokenized'].values
y_train = train_data['label'].values
X_test= test_data['tokenized'].values
y_test = test_data['label'].values

# X_train, y_train, X_test, y_test ì €ì¥

import pickle
with open('shopping_X_train.txt', 'wb') as f:
    pickle.dump(X_train, f)
    
with open('shopping_y_train.txt', 'wb') as f:
    pickle.dump(y_train, f)

with open('shopping_X_test.txt', 'wb') as f:
    pickle.dump(X_test, f)
    
with open('shopping_y_test.txt', 'wb') as f:
    pickle.dump(y_test, f)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

threshold = 2
total_cnt = len(tokenizer.word_index) # ë‹¨ì–´ì˜ ìˆ˜
rare_cnt = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸
total_freq = 0 # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©
rare_freq = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©

# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :',total_cnt)
print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))
print("ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:", (rare_cnt / total_cnt)*100)
print("ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:", (rare_freq / total_freq)*100)

# ì „ì²´ ë‹¨ì–´ ê°œìˆ˜ ì¤‘ ë¹ˆë„ìˆ˜ 2ì´í•˜ì¸ ë‹¨ì–´ ê°œìˆ˜ëŠ” ì œê±°.
# 0ë²ˆ íŒ¨ë”© í† í°ê³¼ 1ë²ˆ OOV í† í°ì„ ê³ ë ¤í•˜ì—¬ +2
vocab_size = total_cnt - rare_cnt + 2
print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° :',vocab_size)


tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') 
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

print('ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ :',max(len(l) for l in X_train))
print('ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ :',sum(map(len, X_train))/len(X_train))
plt.hist([len(s) for s in X_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

def below_threshold_len(max_len, nested_list):
  cnt = 0
  for s in nested_list:
    if(len(s) <= max_len):
        cnt = cnt + 1
  print('ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: %s'%(max_len, (cnt / len(nested_list))*100))

max_len = 80
below_threshold_len(max_len, X_train)

X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)

import pickle
with open('shopping_X_train.txt', 'wb') as f:
    pickle.dump(X_train, f)
    
with open('shopping_X_test.txt', 'wb') as f:
    pickle.dump(X_test, f)

from tensorflow.keras.layers import Embedding, Dense, GRU
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint


model = Sequential()
model.add(Embedding(vocab_size, 100))
model.add(GRU(128))
model.add(Dense(1, activation='sigmoid'))
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\

loaded_model = load_model('naver_shopping_best_model.h5')
print("\n í…ŒìŠ¤íŠ¸ ì •í™•ë„: %.4f" % (loaded_model.evaluate(X_test, y_test)[1]))

def sentiment_predict(new_sentence):
  new_sentence = mecab.morphs(new_sentence) # í† í°í™”
  new_sentence = [word for word in new_sentence if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°
  encoded = tokenizer.texts_to_sequences([new_sentence]) # ì •ìˆ˜ ì¸ì½”ë”©
  pad_new = pad_sequences(encoded, maxlen = max_len) # íŒ¨ë”©
  score = float(loaded_model.predict(pad_new)) # ì˜ˆì¸¡
  if(score > 0.5):
    print("{:.2f}% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.".format(score * 100))
  else:
    print("{:.2f}% í™•ë¥ ë¡œ ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤.".format((1 - score) * 100))

sentiment_predict('ì´ ìƒí’ˆ ì§„ì§œ ì¢‹ì•„ìš”... ì €ëŠ” ê°•ì¶”í•©ë‹ˆë‹¤. ëŒ€ë°•')
sentiment_predict('ì§„ì§œ ë°°ì†¡ë„ ëŠ¦ê³  ê°œì§œì¦ë‚˜ë„¤ìš”. ë­ ì´ëŸ° ê±¸ ìƒí’ˆì´ë¼ê³  ë§Œë“¬?')
sentiment_predict('íŒë§¤ìë‹˜... ë„ˆë¬´ ì§±ì´ì—ìš”.. ëŒ€ë°•ë‚˜ì‚¼')
sentiment_predict('ã…ã„´ã…‡ã„»ã„´ã…‡ã„»ã„´ã…‡ë¦¬ë·°ì“°ê¸°ë„ ê·€ì°®ì•„')

crawling_df = pd.read_csv('Copy of á„‰á…®á†«á„‹á…±á„‡á…§á†¯á„’á…¡á†«á„Œá…®á†¯á„‘á…§á†¼_á„Œá…©á†¼á„’á…¡á†¸.csv')
crawling_df = crawling_df['í•œì¤„í‰'] 

for str in crawling_df.iloc[: 20]:
    print( str ,'-----ë¶„ì„ê²°ê³¼---->', end='')
    sentiment_predict(str)

    print()

"""## ì‡¼í•‘ëª° ë¦¬ë·° í† í¬ë‚˜ì´ì§•, ëª¨ë¸ ë§Œë“ ê²ƒì„ ë°”ë¡œ ì ìš©í•´ì£¼ê²Œ í†µí•©
* ìœ„ì˜ í•™ìŠµ ê³¼ì •ì„ ê±°ì¹˜ê³  ì´ ë¸”ë¡ì„ ì‹¤í–‰í•˜ë©´ ë¨.
* ì¶”í›„ì— ë„ì„œ ë¦¬ë·°ë°ì´í„°ë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ë¡œ ë§Œë“¦
* ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆê²Œ êµ¬ì„±
* ì´ ëª¨ë“ˆì„ í™œìš©í•´ì„œ ê° ë„ì„œ ëŒ“ê¸€ì„ ê°ì„±ë¶„ì„í•˜ê³  ê°ì„± ë¶„ì„ í•œ ê²°ê³¼ë¥¼ ì €ì¥
* ì´ ê²°ê³¼ë¥¼ í† ëŒ€ë¡œ ì‚¬ìš©ìê°€ í•´ë‹¹ ë„ì„œë¥¼ ì…ë ¥í–ˆì„ ê²½ìš° í•´ë‹¹ ë„ì„œì˜ ê¸ë¶€ì • í¼ì„¼íŠ¸ë¥¼ ë³´ì—¬ì¤Œ

"""

# Colabì— Mecab ì„¤ì¹˜
#!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
#%cd Mecab-ko-for-Google-Colab
#!bash install_mecab-ko_on_colab190912.sh


# í•™ìŠµëœ ëª¨ë¸ ì ìš©

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from collections import Counter
from konlpy.tag import Mecab
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import os

os.chdir(drive_path)
print(os.getcwd())

total_data = pd.read_table('shopping_ratings_total.txt', names=['ratings', 'reviews'])
display(total_data)
print('ì „ì²´ ë¦¬ë·° ê°œìˆ˜ :',len(total_data)) # ì „ì²´ ë¦¬ë·° ê°œìˆ˜ ì¶œë ¥

total_data['label'] = np.select([total_data.ratings > 3], [1], default=0)

train_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)
print('í›ˆë ¨ìš© ë¦¬ë·°ì˜ ê°œìˆ˜ :', len(train_data))
print('í…ŒìŠ¤íŠ¸ìš© ë¦¬ë·°ì˜ ê°œìˆ˜ :', len(test_data))

# í•œê¸€ê³¼ ê³µë°±ì„ ì œì™¸í•˜ê³  ëª¨ë‘ ì œê±°
train_data['reviews'] = train_data['reviews'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")
train_data['reviews'].replace('', np.nan, inplace=True)
print(train_data.isnull().sum())

test_data.drop_duplicates(subset = ['reviews'], inplace=True) # ì¤‘ë³µ ì œê±°
test_data['reviews'] = test_data['reviews'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","") # ì •ê·œ í‘œí˜„ì‹ ìˆ˜í–‰
test_data['reviews'].replace('', np.nan, inplace=True) # ê³µë°±ì€ Null ê°’ìœ¼ë¡œ ë³€ê²½
test_data = test_data.dropna(how='any') # Null ê°’ ì œê±°
print('ì „ì²˜ë¦¬ í›„ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œì˜ ê°œìˆ˜ :',len(test_data))

mecab = Mecab()
stopwords = ['ë„', 'ëŠ”', 'ë‹¤', 'ì˜', 'ê°€', 'ì´', 'ì€', 'í•œ', 'ì—', 'í•˜', 'ê³ ', 'ì„', 'ë¥¼', 'ì¸', 'ë“¯', 'ê³¼', 'ì™€', 'ë„¤', 'ë“¤', 'ë“¯', 'ì§€', 'ì„', 'ê²Œ']

with open('shopping_X_train.txt', 'rb') as f:
    X_train = pickle.load(f) # ë‹¨ í•œì¤„ì”© ì½ì–´ì˜´


with open('shopping_X_test.txt', 'rb') as f:
    X_test = pickle.load(f) # ë‹¨ í•œì¤„ì”© ì½ì–´ì˜´

with open('shopping_y_train.txt', 'rb') as f:
    y_train = pickle.load(f) # ë‹¨ í•œì¤„ì”© ì½ì–´ì˜´


with open('shopping_y_test.txt', 'rb') as f:
    y_test = pickle.load(f) # ë‹¨ í•œì¤„ì”© ì½ì–´ì˜´


tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

threshold = 2
total_cnt = len(tokenizer.word_index) # ë‹¨ì–´ì˜ ìˆ˜
rare_cnt = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸
total_freq = 0 # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©
rare_freq = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©

# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :',total_cnt)
print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))
print("ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:", (rare_cnt / total_cnt)*100)
print("ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:", (rare_freq / total_freq)*100)



# ì „ì²´ ë‹¨ì–´ ê°œìˆ˜ ì¤‘ ë¹ˆë„ìˆ˜ 2ì´í•˜ì¸ ë‹¨ì–´ ê°œìˆ˜ëŠ” ì œê±°.
# 0ë²ˆ íŒ¨ë”© í† í°ê³¼ 1ë²ˆ OOV í† í°ì„ ê³ ë ¤í•˜ì—¬ +2
vocab_size = total_cnt - rare_cnt + 2
print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° :',vocab_size)


tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') 
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)


def below_threshold_len(max_len, nested_list):
  cnt = 0
  for s in nested_list:
    if(len(s) <= max_len):
        cnt = cnt + 1
  print('ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: %s'%(max_len, (cnt / len(nested_list))*100))

max_len = 80
below_threshold_len(max_len, X_train)

X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)


from tensorflow.keras.layers import Embedding, Dense, GRU
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint



loaded_model = load_model('naver_shopping_best_model.h5')

def sentiment_predict(new_sentence):
  new_sentence = mecab.morphs(new_sentence) # í† í°í™”
  new_sentence = [word for word in new_sentence if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°
  encoded = tokenizer.texts_to_sequences([new_sentence]) # ì •ìˆ˜ ì¸ì½”ë”©
  pad_new = pad_sequences(encoded, maxlen = max_len) # íŒ¨ë”©
  score = float(loaded_model.predict(pad_new)) # ì˜ˆì¸¡
  if(score > 0.5):
    print("{:.2f}% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.".format(score * 100))
    return score
  else:
    print("{:.2f}% í™•ë¥ ë¡œ ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤.".format((1 - score) * 100))
    return (1-score)



sentiment_predict('ì´ ìƒí’ˆ ì§„ì§œ ì¢‹ì•„ìš”... ì €ëŠ” ê°•ì¶”í•©ë‹ˆë‹¤. ëŒ€ë°•')
sentiment_predict('ì§„ì§œ ë°°ì†¡ë„ ëŠ¦ê³  ê°œì§œì¦ë‚˜ë„¤ìš”. ë­ ì´ëŸ° ê±¸ ìƒí’ˆì´ë¼ê³  ë§Œë“¬?')
sentiment_predict('íŒë§¤ìë‹˜... ë„ˆë¬´ ì§±ì´ì—ìš”.. ëŒ€ë°•ë‚˜ì‚¼')
sentiment_predict('ã…ã„´ã…‡ã„»ã„´ã…‡ã„»ã„´ã…‡ë¦¬ë·°ì“°ê¸°ë„ ê·€ì°®ì•„')

"""# ì˜í™” ë¦¬ë·° ê¸ì • ë¶€ì • ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ ê¸ ë¶€ì •ì„ í‰ê°€í•˜ê³  ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì»¨í…ì¸  ê¸°ë°˜ í•„í„°ë§ ì¶”ì²œ ê¸°ë²• ì‚¬ìš©"""

# í•„ìš”í•œ ëª¨ë“ˆê³¼ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
from warnings import filterwarnings
filterwarnings('ignore')

"""## ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"""

book_senti_df = pd.read_csv(drive_path + '/á„ƒá…©á„‰á…¥á„€á…¡á†·á„‰á…¥á†¼á„ƒá…¦á„‹á…µá„á…¥.csv') 
crawling_df = pd.read_csv(drive_path + '/Copy of á„‰á…®á†«á„‹á…±á„‡á…§á†¯á„’á…¡á†«á„Œá…®á†¯á„‘á…§á†¼_á„Œá…©á†¼á„’á…¡á†¸.csv')

display(book_senti_df)
display(crawling_df)
display(crawling_df.info())

# í¬ë¡¤ë§ í•œ ë°ì´í„° í‰ì ì— ë”°ë¼ 0, 1 (ë¶€ì •, ê¸ì •)ìœ¼ë¡œ ìˆ˜ì • ë° ì €ì¥
indexing = {'í‰ì 1ì ': 0, 'í‰ì 2ì ': 0, 'í‰ì 3ì ': 1, 'í‰ì 4ì ': 1, 'í‰ì 5ì ': 1}

# í•œê¸€ê³¼ ê³µë°±ì„ ì œì™¸í•˜ê³  ëª¨ë‘ ì œê±°
crawling_df['í•œì¤„í‰'] = crawling_df['í•œì¤„í‰'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")
crawling_df['í•œì¤„í‰'].replace('', np.nan, inplace=True)
print(crawling_df.isnull().sum())
crawling_df = crawling_df.dropna(how='any') # Null ê°’ ì œê±°

senti = []
int_score = []
for score in crawling_df['í‰ì ']:
    
    if score == 'í‰ì 5ì ':
        senti.append(1)
        int_score.append(5)
    if score == 'í‰ì 4ì ':
        senti.append(1)
        int_score.append(4)
    elif score == 'í‰ì 3ì ':
        senti.append(0)
        int_score.append(3)
    elif score == 'í‰ì 2ì ':
        senti.append(0)
        int_score.append(2)
    elif score == 'í‰ì 1ì ':
        senti.append(0)
        int_score.append(1)
    

crawling_df['ê°ì •'] = senti
crawling_df['í‰ì '] = int_score

display(crawling_df)
# ë„ì„œ ê°ì„± ë°ì´í„° 0, 1ë¡œ ë‚˜ëˆ„ê¸°
# ë‘˜ì´ í•©ì¹˜ê¸°
# ë„¤ì´ë²„ ì‡¼í•‘ í›„ê¸° ë¶„ì„ í•œê²ƒ ì²˜ëŸ¼ ê³ ì¹˜ê¸°

"""## ê¸ë¶€ì • ë°ì´í„° ì¶”ê°€"""

positive = []
negative = []

for str in crawling_df['í•œì¤„í‰']:
    p, n = sentiment_predict(str)
    print(p, n)
    positive.append(p)
    negative.append(n)
crawling_df['ê¸ì •í™•ë¥ '] = positive
crawling_df['ë¶€ì •í™•ë¥ '] = negative

display(crawling_df)

crawling_df.to_csv(drive_path + 'ë„ì„œê¸ë¶€ì •í™•ë¥ í¬í•¨.csv')

# í•„ìš”í•œ ëª¨ë“ˆê³¼ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
from warnings import filterwarnings
import re
from sklearn.feature_extraction.text import CountVectorizer

filterwarnings('ignore')

"""## ë¶„ë¥˜ íŠ¹ì„± ë°ì´í„° ì •ì œí›„ ë¶„ë¥˜ì—´ ì†ì„± ì¶”ê°€"""

data = pd.read_csv(drive_path + 'ë„ì„œê¸ë¶€ì •í™•ë¥ í¬í•¨.csv')

from sklearn.feature_extraction.text import CountVectorizer

data['ë¶„ë¥˜ì—´'] = data['ë¶„ë¥˜'].apply( lambda x : x[1:-1].replace(',',' '))
# íŠ¹ìˆ˜ ë¬¸ì œ ì œê±°
data["ë¶„ë¥˜ì—´"] = data["ë¶„ë¥˜ì—´"].str.replace(pat=r'[^\w]', repl=r' ', regex=True)


data = data.iloc[:, 2: ]
display(data)
display(data['ë¶„ë¥˜ì—´'])

"""## ì±… id, name, ìˆœìœ„ë¡œ ì±…ì„ ê·¸ë£¹í™” í•˜ê³  ê¸ë¶€ì •í™•ë¥  í‰ê· ì„ êµ¬í•¨ """

#ìˆœìœ„ë³„í‰ê· í™•ë¥  csvë¡œ ë§Œë“¤ê¸°
mean_data = data.groupby(['id', 'name', 'ìˆœìœ„'])['ê¸ì •í™•ë¥ ','ë¶€ì •í™•ë¥ '].mean()
mean_data.reset_index(inplace=True)
mean_data.sort_values('ìˆœìœ„')
mean_data.to_csv(drive_path + 'ìˆœìœ„ë³„í‰ê· í™•ë¥ í‰ê· .csv')
display(mean_data)

"""## ë¶„ë¥˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°"""

# ë¶„ë¥˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°í•´ì£¼ëŠ” ì½”ë“œ
ex_data = data.drop_duplicates(['name'])
ex_data = ex_data.iloc[:, :][['id', 'name', 'ë¶„ë¥˜ì—´']]

ex_data.to_csv(drive_path + 'ë¶„ë¥˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°.csv')
display(ex_data)

"""## ê° ì‘í’ˆë³„ í‰ê°€í•œ ì‚¬ëŒìˆ˜ """

# ê° ì‘í’ˆë³„ í‰ê°€í•œ ì‚¬ëŒ ìˆ˜
count_score_data = data.groupby(['id','name','ìˆœìœ„'])['í•œì¤„í‰'].count()
display(count_score_data)
count_score_data.to_csv(drive_path + 'ê°ì‘í’ˆë³„í‰ê°€í•œì‚¬ëŒìˆ˜')

count_score_data = pd.read_csv(drive_path + 'ê°ì‘í’ˆë³„í‰ê°€í•œì‚¬ëŒìˆ˜')
count_score_data.sort_values('ìˆœìœ„')

"""## ê° ì‘í’ˆë³„ í‰ê·  í‰ì  êµ¬í•˜ê¸°"""

# ê° ì‘í’ˆë³„ í‰ê·  í‰ì 
mean_score_data = data.groupby(['id','name','ìˆœìœ„'])['í‰ì '].mean()
display(mean_score_data)
mean_score_data.to_csv(drive_path + 'ê°ì‘í’ˆë³„í‰ê· í‰ì ')

mean_score_data = pd.read_csv(drive_path + 'ê°ì‘í’ˆë³„í‰ê· í‰ì ')
mean_score_data.sort_values('ìˆœìœ„')
mean_score_data

"""## ë°ì´í„° ì¡°ì¸í•˜ê¸°(ì¼ë‹¨ ë‹¤ í•©ì¹œê±°ë¼ ì˜¤ë¥˜ê°€ ìˆìŒ)"""

# mean_score_data  +  count_score_data + ex_data + mean_data
merge_data1 = pd.merge(mean_score_data, count_score_data, how='inner', on = 'id')
merge_data2 = pd.merge(ex_data, mean_data, how='inner', on = 'id')
merge_data = pd.merge(merge_data1, merge_data2, how='inner', on = 'id')
display(merge_data)
merge_data.columns = ['id','name','ìˆœìœ„_x' ,'í‰ì ','name_y_x','ìˆœìœ„_y',	'í•œì¤„í‰','name_x_y','ë¶„ë¥˜ì—´','name_y_y','ìˆœìœ„','ê¸ì •í™•ë¥ ','ë¶€ì •í™•ë¥ ']
merge_data = merge_data[['id', 'name', 'í•œì¤„í‰', 'ìˆœìœ„', 'ë¶„ë¥˜ì—´', 'ê¸ì •í™•ë¥ ', 'ë¶€ì •í™•ë¥ ', 'í‰ì ']].sort_values('ìˆœìœ„')

merge_data = merge_data.drop_duplicates(['name'])
merge_data['name'] = merge_data['name'].str.split().agg("".join)
merge_data.to_csv(drive_path +'ê°€ì¤‘ì¹˜ì—í•„ìš”í•œë°ì´í„°ëª¨ìŒ.csv')
display(merge_data)

"""## ì»¨í…ì¸  ê¸°ë°˜ ì¶”ì²œ ë¶€ë¶„
* ìœ„ì˜ ì¡°ì¸í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰
* ì„œë¹„ìŠ¤ë¡œ ì˜¬ë¦´ë•ŒëŠ” ì¡°ì¸í•œ ë°ì´í„°ë§Œ ìˆìœ¼ë©´ ë ê²ƒ ê°™ìŒ.
* ì¼ë‹¨ í”„ë¡ íŠ¸ë‹¨ì„ êµ¬í˜„í•˜ê³  ì‹œê°„ì´ ë‚¨ìœ¼ë©´ ì´ ë°ì´í„°ë¥¼ ë””ë¹„ë¡œ êµ¬ì¶•í•˜ê³  ë°°í¬í•˜ë©´ ë ë“¯.
"""

# í•„ìš”í•œ ëª¨ë“ˆê³¼ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
from warnings import filterwarnings
import re
from sklearn.feature_extraction.text import CountVectorizer

filterwarnings('ignore')

merge_data = pd.read_csv(drive_path +'/ê°€ì¤‘ì¹˜ì—í•„ìš”í•œë°ì´í„°ëª¨ìŒ.csv')

count_vect = CountVectorizer(min_df=0, ngram_range=(1,2))
genre_mat = count_vect.fit_transform( merge_data['ë¶„ë¥˜ì—´'] )
print(genre_mat.shape)

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
# ë°˜í™˜ëœ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ì˜ í¬ê¸° ë° ì• 2ê°œ ë°ì´í„°ë§Œ ì¶”ì¶œ
from sklearn.metrics.pairwise import cosine_similarity

genre_sim = cosine_similarity(genre_mat, genre_mat)
genre_sim.shape

genre_sim[:2]
np.sort(genre_sim)[:, ::-1]


# ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë¦¬ëœ genre_sim ê°ì²´ì˜ ë¹„êµ í–‰ ìœ„ì¹˜ ì¸ë±ìŠ¤ ê°’
# ê°’ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë¹„êµ ëŒ€ìƒ í–‰ì˜ ìœ ì‚¬ë„ ê°’ì´ ì•„ë‹ˆë¼
#  ë¹„êµ ëŒ€ìƒ í–‰ì˜ ìœ„ì¹˜ ì¸ë±ìŠ¤ì„ì— ì£¼ì˜
genre_sim_sorded_ind = genre_sim.argsort()[:, ::-1]
genre_sim_sorded_ind[:1]

# ì¥ë¥´ ìœ ì‚¬ë„ì— ë”°ë¼ ì˜í™”ë¥¼ ì¶”ì²œí•˜ëŠ” í•¨ìˆ˜ë¥¼ ìƒì„±
# movies_df DataFrame, 
# ë ˆì½”ë“œë³„ ì¥ë¥´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì¸ë±ìŠ¤ë¥¼ ê°€ì§€ëŠ” genre_sim_sorted_ind
# ê³ ê°ì´ ì„ ì •í•œ ì¶”ì²œ ê¸°ì¤€ì´ ë˜ëŠ” ì˜í™” ì œëª©
# ì¶”ì²œí•  ì˜í™” ê±´ìˆ˜
# return : ì¶”ì²œ ì˜í™” ì •ë³´ DataFrame

# ì •í™•í•œ ì±…ì´ë¦„ìœ¼ë¡œ ì°¾ê¸°
def find_sim_book(df, sorted_ind, title_name, top_n=10):
  
  
  title_book = df[ df['name'].str.contains(title_name) ]
  title_book = title_book[title_book['í‰ì '] == title_book['í‰ì '].max() ]
  display(title_book)

  title_index = title_book.index.values
  sim_indexs = sorted_ind[title_index, :(top_n)]
  sim_indexs = sim_indexs.reshape(-1)
  sim_indexs = sim_indexs[sim_indexs != title_index]

  return df.iloc[sim_indexs].sort_values('weighted_vote', 
                                         ascending=False)[:top_n]

# í‚¤ì›Œë“œë¡œ ì°¾ê¸°
def find_keyword_book(df, sorted_ind, keyword, top_n=10):
  
  keyword_book = pd.DataFrame()
  for key in keyword[1:]:

    temp = df[ df['ë¶„ë¥˜ì—´'].str.contains(key) ]
    keyword_book = pd.concat([temp,keyword_book])

  keyword_book.drop_duplicates(['id'])

  return keyword_book.sort_values('weighted_vote', 
                                         ascending=False)[:top_n]

# ê°€ì¤‘í‰ì =(ğ‘£/(ğ‘£+ğ‘š+p))âˆ—ğ‘…+(ğ‘š/(ğ‘£+ğ‘š+p))âˆ—ğ¶ + (p/(ğ‘£+ğ‘š+p))*P*3

# v: ê°œë³„ ì˜í™”ì— í‰ì ì„ íˆ¬í‘œí•œ íšŸìˆ˜
# m: í‰ì ì„ ë¶€ì—¬í•˜ê¸° ìœ„í•œ ìµœì†Œ íˆ¬í‘œ íšŸìˆ˜
# p: ê¸ì •í‰ê°€ í™•ë¥ 
# R: ê°œë³„ ì±…ì— ëŒ€í•œ í‰ê·  í‰ì 
# C: ì „ì²´ ì±…ì— ëŒ€í•œ í‰ê·  í‰ì 
# P: ì „ì²´ ì±…ì— ëŒ€í•œ í‰ê·  ê¸ì •

# ê¸°ì¡´ í‰ì ì„ ê°€ì¤‘ í‰ì ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜

C = merge_data['í‰ì '].mean()
m = merge_data['í•œì¤„í‰'].quantile(0.6)
p = merge_data['ê¸ì •í™•ë¥ '].mean()
    
v = merge_data['í•œì¤„í‰']
R = merge_data['í‰ì ']
P = merge_data['ê¸ì •í™•ë¥ ']

merge_data['weighted_vote'] = (v/(v+m+p))*R+(m/(v+m+p))*C + (p/(v+m+p))*P*5
display(merge_data)

# test code
choice = input("ì œëª©ìœ¼ë¡œ ì°¾ê¸°: 1ë²ˆ or í‚¤ì›Œë“œë¡œ ì°¾ê¸°: 2ë²ˆ or ì±… ìˆœìœ„ ë³´ê¸° 3ë²ˆ\n")

# ì œëª©ìœ¼ë¡œ ì°¾ì•„ë³´ê¸°(ì œëª©ì´ ì •í™•í•´ì•¼í•¨)
if choice == '1':

    user_book = input("ì •í™•í•œ ì±… ì œëª©ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    # ë„ì–´ì“°ê¸° ì œê±°
    user_book = user_book.replace(' ','')
    print(user_book)
    sim_books = find_sim_book(merge_data, genre_sim_sorded_ind, user_book, 10) # ì±… ì´ë¦„ì„ ì…ë ¥

# í‚¤ì›Œë“œë¡œ ì°¾ì•„ë³´ê¸°
elif choice == '2':

    user_book = input("ì±… í‚¤ì›Œë“œë¥¼ #ë¡œ ì…ë ¥í•˜ì„¸ìš”: ")
    user_book = user_book.replace(' ','')

    print(user_book)
    keyword = user_book.split('#')
    print(keyword)
    sim_books = find_keyword_book(merge_data, genre_sim_sorded_ind, keyword, 10) # ì±… ì´ë¦„ì„ ì…ë ¥

elif choice == '3':
    bound = input('ì›í•˜ëŠ” ìˆœìœ„ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ex)1-10(10ë‹¨ìœ„) \n')
    bound = bound.replace(' ', '')
    bound = bound.split('-')
    print(bound)
    merge_data['ìˆœìœ„'] = merge_data['ìˆœìœ„'].astype('int') 
    sim_books = merge_data[(merge_data['ìˆœìœ„'] >= int(bound[0])) & (merge_data['ìˆœìœ„'] <= int(bound[1]))]
else:
     print('ì˜ëª»ëœ ì„ íƒì§€ ì…ë ¥ ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.')

sim_books = sim_books[['id', 'name', 'í•œì¤„í‰', 'ìˆœìœ„', 'ë¶„ë¥˜ì—´', 'ê¸ì •í™•ë¥ ', 'ë¶€ì •í™•ë¥ ', 'í‰ì ', 'weighted_vote']]
display(sim_books) # ì¶”ì²œëœ ì±… ë³´ì—¬ì¤Œ

